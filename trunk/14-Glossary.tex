\documentclass[MachineLearning]{subfiles}
\begin{document}
\section{Glossary}
\renewcommand{\arraystretch}{1.5}
\definecolor{lgray}{gray}{0.95}
\definecolor{gray}{gray}{0.9}

\rowcolors{1}{lgray}{gray}

\begin{longtable}{p{0.3\linewidth} p{0.7\linewidth}}
\hline \textbf{Term} & \textbf{Brief explanation of the term}\\ \hline
\endfirsthead

\hline \textbf{Term} & \textbf{Brief explanation of the term}\\ \hline\hline
\endhead
A very important term & The much longer explanations breaking over several lines here and more text to see the line breaking again and again and again and again.\\
Classification &  \\

Regression &  \\

Structured Prediction &  \\

Supervised Learning &  \\

Unsupervised Learning & 'Learning without labels' \newline
Usual goals:\newline
\begin{enumerate}
\item Compact representation of data sets (e.g. via clustering / quantization)
\item Dimension reduction (e.g.; very useful for visualization)
\item Anomaly detection of ''unusual'' data points
\end{enumerate} \\

Clustering & \\

Dimension reduction & \\

Anomaly detection & \\

Feature & \\

Model selection & \\

Model validation & \\
Nominal or categorical scale & Qualitative, but without quantitative measurements,
e.g. binary scale \(\mathbb{X} = \{0, 1\}\) (presence or absence of
properties). Ordering does not matter.\\
Ordinal scale & Measurement values are meaningful only with respect to other measurements, i.e., the rank order of measurements carries
the information, not the numerical differences {\color{orange}(\emph{e.g. information on the ranking of different marathon races)}}\\
Interval scale &  The relation of numerical differences carries
the information. Invariance w.r.t. translation and scaling {\color{orange}\emph{(Fahrenheit scale of temperature)}}.\\
Ratio scale & Zero value of the scale carries information but not the measurement unit. {\color{orange}\emph{(Kelvin scale)}}.\\
Absolute scale & Absolute values are meaningful. {\color{orange}\emph{(grades of final exams)}}\\
Data Whitening & Normalize the values of a feature vector by the standard deviation (or another scale quantity) in this component. Thereby, differences in dynamic range (e.g., by different measurement units) are eliminated.\\
Homogeneous coordinates \label{homogeneous-coordinates@Glossary} & Suppose we have a point (x,y) in the Euclidean plane. To represent this same point in the projective plane, we simply add a third coordinate of 1 at the end: (x, y, 1) Overall scaling is unimportant, so the point (x,y,1) is the same as the point  \((\alpha x, \alpha y, \alpha)\), for any nonzero \(\alpha\). In other words, 
\[(X,Y,W) = (\alpha X, \alpha Y, \alpha W)\]
for any  $\alpha \neq 0$ (Thus the point (0,0,0) is disallowed). Because scaling is unimportant, the coordinates (X,Y,W) are called the homogeneous coordinates of the point. (Stanford Robotics: \url{http://robotics.stanford.edu/~birch/projective/node4.html})\\

Singular value decomposition \label{SVD@Glossary} &  The singular value decomposition (SVD) is a factorization of a matrix. The singular value decomposition of an \(m \times n\) matrix \(X\) is a factorization of the form \(X = UDV^T\), where \(U\) is an \(m \times m\) orthogonal matrix, \(D\) is an \(m \times n\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \(V^T\) is an \(n \times n\) orthogonal matrix. The diagonal entries \(D_i\) are known as the singular values of \(X\). The m columns of \(U\) and the n columns of \(V\) are called the left-singular vectors and right-singular vectors of \(X\), respectively. (SVD MIT Open courseware/ Computing the Singular Value Decomposition: \url{http://youtu.be/cOUTpqlX-Xs})
\end{longtable}
\end{document}