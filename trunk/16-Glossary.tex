\documentclass[MachineLearning]{subfiles}
\begin{document}
\section{Glossary}
\renewcommand{\arraystretch}{1.5}
\definecolor{lgray}{gray}{0.95}
\definecolor{gray}{gray}{0.9}

\rowcolors{1}{lgray}{gray}

\begin{longtable}{p{0.3\linewidth} p{0.7\linewidth}}
\hline \textbf{Term} & \textbf{Brief explanation of the term}\\ \hline
\endfirsthead

\hline \textbf{Term} & \textbf{Brief explanation of the term}\\ \hline\hline
\endhead
A very important term & The much longer explanations breaking over several lines here and more text to see the line breaking again and again and again and again.\\
Classification &  \\

Regression &  \\

Structured Prediction &  \\

Supervised Learning &  \\

Unsupervised Learning & 'Learning without labels' \newline
Usual goals:\newline
\begin{enumerate}
\item Compact representation of data sets (e.g. via clustering / quantization)
\item Dimension reduction (e.g.; very useful for visualization)
\item Anomaly detection of ''unusual'' data points
\end{enumerate} \\

Clustering & \\

Dimension reduction & \\

Anomaly detection & \\

Feature & \\

Model selection & \\

Model validation & \\
Nominal or categorical scale & Qualitative, but without quantitative measurements,
e.g. binary scale \(\mathbb{X} = \{0, 1\}\) (presence or absence of
properties). Ordering does not matter.\\
Ordinal scale & Measurement values are meaningful only with respect to other measurements, i.e., the rank order of measurements carries
the information, not the numerical differences {\color{orange}(\emph{e.g. information on the ranking of different marathon races)}}\\
Interval scale &  The relation of numerical differences carries
the information. Invariance w.r.t. translation and scaling {\color{orange}\emph{(Fahrenheit scale of temperature)}}.\\
Ratio scale & Zero value of the scale carries information but not the measurement unit. {\color{orange}\emph{(Kelvin scale)}}.\\
Absolute scale & Absolute values are meaningful. {\color{orange}\emph{(grades of final exams)}}\\
Data Whitening & Normalize the values of a feature vector by the standard deviation (or another scale quantity) in this component. Thereby, differences in dynamic range (e.g., by different measurement units) are eliminated.\\
Homogeneous coordinates \label{homogeneous-coordinates@Glossary} & Suppose we have a point (x,y) in the Euclidean plane. To represent this same point in the projective plane, we simply add a third coordinate of 1 at the end: (x, y, 1) Overall scaling is unimportant, so the point (x,y,1) is the same as the point  \((\alpha x, \alpha y, \alpha)\), for any nonzero \(\alpha\). In other words, 
\[(X,Y,W) = (\alpha X, \alpha Y, \alpha W)\]
for any  $\alpha \neq 0$ (Thus the point (0,0,0) is disallowed). Because scaling is unimportant, the coordinates (X,Y,W) are called the homogeneous coordinates of the point. (Stanford Robotics: \url{http://robotics.stanford.edu/~birch/projective/node4.html})\\

Singular value decomposition \label{SVD@Glossary} &  The singular value decomposition (SVD) is a factorization of a matrix. The singular value decomposition of an \(m \times n\) matrix \(X\) is a factorization of the form \(X = UDV^T\), where \(U\) is an \(m \times m\) orthogonal matrix, \(D\) is an \(m \times n\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \(V^T\) is an \(n \times n\) orthogonal matrix. The diagonal entries \(D_i\) are known as the singular values of \(X\). The m columns of \(U\) and the n columns of \(V\) are called the left-singular vectors and right-singular vectors of \(X\), respectively. (SVD MIT Open courseware/ Computing the Singular Value Decomposition: \url{http://youtu.be/cOUTpqlX-Xs})\\
Norms (\(L_1,L_2\) norm) \label{Norms@Glossary}& Norms determine the length of a vector in a certain space. In the Euclidian space we use the \(L_2\) norm which is \[\|x\|_2=\sqrt{\left(x_1^2+x_2^2+\dotsb+x_n^2\right)}\] whereas if we want to measure the grid distance, we use the \(L_1\) norm or ''Manhattan distance'' which is \[ \|x\|_1=\left(|x_1|+|x_2|+\dotsb+|x_n|\right)\]\\
Regularization & Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. This information is usually of the form of a penalty for complexity, such as restrictions for smoothness or bounds on the vector space norm.\\
Likelihood function \(\log P(\theta; X)\) & In statistics, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model. Likelihood functions play a key role in statistical inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, "likelihood" is often used as a synonym for "probability." But in statistical usage, a distinction is made depending on the roles of the outcome or parameter. Probability is used when describing a function of the outcome given a fixed parameter value. For example, if a coin is flipped 10 times and it is a fair coin, what is the probability of it landing heads-up every time? Likelihood is used when describing a function of a parameter given an outcome. For example, if a coin is flipped 10 times and it has landed heads-up 10 times, what is the likelihood that the coin is fair? (Wikipedia Likelihood function: \url{https://en.wikipedia.org/wiki/Likelihood_function})\\
Score function \(\Lambda = \frac{\partial \log P(x;\theta)}{\partial\theta}\) &  The score function indicates how sensitively a likelihood function \(\log P(\theta; X)\) depends on its parameter \(\theta\). Explicitly, the score for \(\theta\) is the gradient of the log-likelihood with respect to \(\theta\).
\end{longtable}
\todo[inline]{Add more terms to the glossary. Link them to the word they describe in the text by using \(\backslash\)hyperref[label name]\{labelID\} to make a link to the position in the text marked by \(\backslash\)label\{labelID\}.}
\end{document}